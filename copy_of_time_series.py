# -*- coding: utf-8 -*-
"""Copy of TIME_SERIES.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1x5LAG2SHw4Lqgt3S8RJfSsDbmk7A4CG5

### Connecting google drive a a working directory

National Grid Dataset

https://drive.google.com/drive/folders/1qcD9tq4yeNSzAQp5Pi_GeHRSJ9n9u78u?usp=sharing

*Download the dataset and store it in any folder inside the google drive*
"""

from google.colab import drive
drive.mount('/content/drive')

"""**Importing the libraries pandas, Numpy and matplotlib**


> pandas for handling tabular datasets


> Numpy for performing math operations and handling datastructures of array format.


> matplotlib for plotting graphs and visualization

### importing the data and analysing
"""

import pandas as pd
import numpy as np

import matplotlib.pyplot as plt

"""**Getting the values for month and year**"""

YEAR = int(input('Enter the year for finding the Dataset ---> '))
MONTH = int(input('Enter the month for finding the Dataset---> '))

"""**location of the dataset**

click the folder icon on the left side of the colab notebook. Go to drive to locate your file and get the path of the national grid data locaated in the drive and paste it hear upto the folder which contains the file. the link given here is **f'/content/drive/MyDrive/DATASETS/fNew {YEAR} {MONTH}.csv'**. copy the link and paste it till the **DATASETS/**
"""

dataset = f'/content/drive/MyDrive/DATASETS/fNew {YEAR} {MONTH}.csv'

"""**Reading the dataset from the format .csv**"""

df = pd.read_csv(dataset)

"""**Splitting data and time in a seperate column**"""

df[['Date','time']] = df.dtm.str.split(" ",expand=True,)

df[['YEAR','MONTH', 'DATE']] = df.Date.str.split("-",expand=True,)
df[['HOURS','MINUTES', 'SECONDS']] = df.time.str.split(":",expand=True,)

df

"""**Rearranging the dataset in an order**"""

df = df[['DATE', 'MONTH', 'YEAR', 'HOURS', 'MINUTES', 'SECONDS', 'f']]

df

"""### Inference of data for ARIMA

**Since it is an ARIMA dataset, the rolling mean and standard deviation is calculated and plotted against the true dataset**
"""

rolmean = df.rolling(window=60).mean()
rolstd = df.rolling(window=12).std()

plt.style.use('ggplot')
plt.plot(df[['f']].head(1000), color='blue')
plt.plot(rolmean[['f']].head(1000), color='red')
# plt.plot(rolstd[['f']].head(5000), color='blue')

from statsmodels.tsa.stattools import adfuller
import pandas.util.testing as tm

"""**Dickeyâ€“Fuller test**

To keep it simple the p-value should be less than 1.
"""

dftest = adfuller(df['f'].head(1000), autolag = 'AIC')
dfoutput = pd.Series(dftest[0:4], index = ['Test statistic', 'p-value', '#Lags Used', 'Number of Observations Used'])
for key,value in dftest[4].items():
  dfoutput['Critical Value (%s)'%key] = value

print(dfoutput)

df_logscale = np.log(df['f'].head(3600))

plt.plot(df_logscale)
# plt.plot(df[['f']].head(1000), color='blue')
# plt.plot(rolmean[['f']].head(1000), color='red')
rolmean = df_logscale.rolling(window=60).mean()
rolstd = df_logscale.rolling(window=12).std()

plt.plot(df_logscale)
plt.plot(rolmean)

df_cos = np.cos(df['f'].tail(100))

plt.plot(df_cos)
plt.plot(np.exp(df_cos)-1.7199*(df_cos))

X = df
X

"""**Scaling the frequency data to the value below 1 so that the mean absolute error is more easily interpreted as a fraction of the maximum demand.**"""

y = df['f']/df['f'].max()

"""**Maximum value of the frequency**"""

df['f'].max()

"""**removing the frequency value from the dataset and storing it a seperate variable so as to determine the future trends of the values using ML** """

X = df[['DATE', 'MONTH', 'YEAR', 'HOURS', 'MINUTES', 'SECONDS']]

X

"""**Scaled frequency values**"""

y

from sklearn.model_selection import TimeSeriesSplit

ts_cv = TimeSeriesSplit(
    n_splits=5,
    gap=48,
    max_train_size=10000,
    test_size=1000,
)

X

from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import OrdinalEncoder
from sklearn.compose import ColumnTransformer
from sklearn.ensemble import HistGradientBoostingRegressor
from sklearn.model_selection import cross_validate


categorical_columns = [
]
categories = [
]
ordinal_encoder = OrdinalEncoder(categories=categories)


gbrt_pipeline = make_pipeline(
    ColumnTransformer(
        transformers=[
            ("categorical", ordinal_encoder, categorical_columns),
        ],
        remainder="passthrough",
    ),
    HistGradientBoostingRegressor(
        categorical_features=range(4),
    ),
)

"""**Model evaluation for Machine Learning**"""

def evaluate(model, X, y, cv):
    cv_results = cross_validate(
        model,
        X,
        y,
        cv=cv,
        scoring=["neg_mean_absolute_error", "neg_root_mean_squared_error"],
    )
    mae = -cv_results["test_neg_mean_absolute_error"]
    rmse = -cv_results["test_neg_root_mean_squared_error"]
    print(
        f"Mean Absolute Error:     {mae.mean():.3f} +/- {mae.std():.3f}\n"
        f"Root Mean Squared Error: {rmse.mean():.3f} +/- {rmse.std():.3f}"
    )

"""### An approoch to predict the future data

**Importing Sklearn libraries for performing ML**
"""

from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import OrdinalEncoder
from sklearn.compose import ColumnTransformer
from sklearn.ensemble import HistGradientBoostingRegressor
from sklearn.model_selection import cross_validate
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import MinMaxScaler
from sklearn.linear_model import RidgeCV
import numpy as np
from sklearn import *
from sklearn.kernel_approximation import Nystroem

"""**Model Creation :- One hot encorder algorithm definition**

"""

one_hot_encoder = OneHotEncoder(handle_unknown="ignore", sparse=False)
alphas = np.logspace(-6, 6, 5)
naive_linear_pipeline = make_pipeline(
    ColumnTransformer(
        transformers=[
            ("categorical", one_hot_encoder, categorical_columns),
        ],
        remainder=MinMaxScaler(),
    ),
    RidgeCV(alphas=alphas),
)

"""**Model Creation :- One hot linear encorder algorithm **"""

one_hot_linear_pipeline = make_pipeline(
    ColumnTransformer(
        transformers=[
            ("categorical", one_hot_encoder, categorical_columns),
            ("one_hot_time", one_hot_encoder, ["YEAR", "MONTH", "HOURS", "MINUTES", "SECONDS"]),
        ],
        remainder=MinMaxScaler(),
    ),
    RidgeCV(alphas=alphas),
)

evaluate(one_hot_linear_pipeline, X, y, cv=ts_cv)

"""**Model Creation :- One hot polynomial encorder algorithm **"""

one_hot_poly_pipeline = make_pipeline(
    ColumnTransformer(
        transformers=[
            ("categorical", one_hot_encoder, categorical_columns),
            ("one_hot_time", one_hot_encoder, ["YEAR", "MONTH", "HOURS", "MINUTES", "SECONDS"]),
        ],
        remainder="passthrough",
    ),
    Nystroem(kernel="poly", degree=2, n_components=75, random_state=0),
    RidgeCV(alphas=alphas),
)
evaluate(one_hot_poly_pipeline, X, y, cv=ts_cv)

"""**Splitting the training and testing dataset for performing ML**"""

all_splits = list(ts_cv.split(X, y))
train_0, test_0 = all_splits[0]

X.iloc[test_0]

train_4, test_4 = all_splits[4]

"""**predicting the test results for one hot linear encorder model** 

"""

one_hot_linear_pipeline.fit(X.iloc[train_0], y.iloc[train_0])
one_hot_linear_predictions = one_hot_linear_pipeline.predict(X.iloc[test_0])

"""**predicting the test results for one hot polynomial encorder model** 

"""

one_hot_poly_pipeline.fit(X.iloc[train_0], y.iloc[train_0])
one_hot_poly_predictions = one_hot_poly_pipeline.predict(X.iloc[test_0])

X.iloc[test_0]

last_hours = slice(-1000, None)
fig, ax = plt.subplots(figsize=(12, 4))
fig.suptitle("Predictions by one hot encoder")
plt.xlabel("Seconds")
plt.xlabel("Frequency")
ax.plot(
    y.iloc[test_0].values[last_hours]*50.258,
    "x-",
    alpha=0.2,
    label="Actual demand",
    color="blue",
)
ax.plot(
    one_hot_poly_predictions[last_hours]*50.303,
    "x-",
    label="One-hot + polynomial kernel",
)
ax.plot(
    one_hot_linear_predictions[last_hours]*50.303,
    "x-",
    label="One-hot + polynomial kernel",
    color="green"
)

"""In the above graph, the blue color is the original dataset and green and red are the predicted frequencies.

Further tuining is required to perfectly fit the model.
"""

from sklearn.metrics import accuracy_score

df['f']

df['failure_1'] = df['f'].apply(lambda x: 1 if (x>=50.015 and x<50.1) else 0)
df['failure_2'] = df['f'].apply(lambda x: 1 if (x>=50.1 and x<50.2) else 0)
df['failure_3'] = df['f'].apply(lambda x: 1 if (x>=50.2 and x<50.3) else 0)
df['failure_4'] = df['f'].apply(lambda x: 1 if (x>=50.3 and x<50.4) else 0)
df['failure_0'] = df['f'].apply(lambda x: 1 if (x>=50.4 and x<50.5) else 0)

"""### case 1: 50.1 < X >= 50.015 Hz


"""

df['failure_1'].sum()

"""### case 2: 50.2 < X >= 50.1 Hz"""

df['failure_2'].sum()

"""### case 3: 50.2 < X >= 50.3 Hz"""

df['failure_3'].sum()

"""### case 4: 50.3 < X >= 50.4 Hz"""

df['failure_4'].sum()

"""### case 5: 50.3 < X >= 50.4 Hz"""

df['failure_0'].sum()

df['failure_5'] = df['f'].apply(lambda x: 1 if (x<=49.985 and x>49.9) else 0)
df['failure_6'] = df['f'].apply(lambda x: 1 if (x<=49.9 and x>49.8) else 0)
df['failure_7'] = df['f'].apply(lambda x: 1 if (x<=49.8 and x>49.7) else 0)
df['failure_8'] = df['f'].apply(lambda x: 1 if (x<=49.7 and x>49.6) else 0)
df['failure_9'] = df['f'].apply(lambda x: 1 if (x<=49.6 and x>49.5) else 0)

"""### case 1*: 49.985 < X >= 49.9 Hz"""

df['failure_5'].sum()

"""### case 2*: 49.9 < X >= 49.8 Hz"""

df['failure_6'].sum()

"""### case 3*: 49.8 < X >= 49.7 Hz"""

df['failure_7'].sum()

"""### case 4*: 49.7 < X >= 49.6 Hz

"""

df['failure_8'].sum()

"""### case 5*: 49.6 < X >= 49.5 Hz"""

df['failure_9'].sum()